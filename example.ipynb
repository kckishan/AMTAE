{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from utils import *\n",
        "import numpy as np\n",
        "from models import MDA, kl_divergence\n",
        "import torch\n",
        "from scipy import io as sio\n",
        "from sklearn.preprocessing import minmax_scale\n",
        "from tqdm import trange\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader\n",
        "from argument_parser import argument_parser\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    args = argument_parser()\n",
        "    table_printer(args)\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available else \"cpu\")\n",
        "\n",
        "    num_nodes = pd.read_csv(args.data_folder + args.dataset + \"/\" +\n",
        "                            args.dataset+\"_string_genes.txt\", header=None).shape[0]\n",
        "\n",
        "    Nets = []\n",
        "    F = []\n",
        "    for net in range(len(args.network_types)):\n",
        "        print(\"Loading network for \", args.network_types[net])\n",
        "        N = sio.loadmat(args.data_folder + args.dataset + \"/\" + args.annotations_path + args.dataset + '_net_' +\n",
        "                        str(net+1) + '_K3_alpha0.98.mat')\n",
        "        Net = N['Net'].todense()\n",
        "        Nets.append(minmax_scale(Net))\n",
        "        F.append(Net.shape[1])\n",
        "\n",
        "    use_sparse = True\n",
        "    BETA = 0.5\n",
        "    tr_x_noisy, tr_x, ts_x_noisy, ts_x = split_data(Nets)\n",
        "    num_networks = len(args.network_types)\n",
        "    z_dim = [args.hidden_size] * num_networks\n",
        "    latent_dim = args.latent_size\n",
        "    model = MDA(F, z_dim, latent_dim)\n",
        "    model.to(device)\n",
        "    print(model)\n",
        "\n",
        "    def count_parameters(model):\n",
        "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(\"Number of trainable parameters:\", count_parameters(model)/1000000)\n",
        "\n",
        "    train_dataset = NetworksDataset(tr_x_noisy, tr_x)\n",
        "    test_dataset = NetworksDataset(ts_x_noisy, ts_x)\n",
        "    trainloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
        "    testloader = DataLoader(test_dataset, batch_size=ts_x_noisy[0].shape[0])\n",
        "    RHO = 0.01\n",
        "    rho = torch.FloatTensor([RHO for _ in range(latent_dim)]).unsqueeze(0).to(device)\n",
        "    criterion = torch.nn.L1Loss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=args.learning_rate,\n",
        "                                momentum=args.momentum, weight_decay=1e-4, nesterov=True)\n",
        "    epochs = trange(args.epochs, desc=\"Validation Loss\")\n",
        "    best_loss = np.Inf\n",
        "    no_improvement = 0\n",
        "    early_stopping_limit = 10\n",
        "    for epoch in epochs:\n",
        "        model.train()\n",
        "        for i, X in enumerate(trainloader):\n",
        "            input_x, output_x = X\n",
        "            input_x = list_to_gpu(input_x, device)\n",
        "            output_x = list_to_gpu(output_x, device)\n",
        "            # ---------------forward------------------\n",
        "            encoded, enc, dec, out = model(input_x)\n",
        "            rec_loss, m_loss = criterion_for_list(criterion, output_x, out)\n",
        "            if use_sparse:\n",
        "                rho_hat = torch.sum(encoded, dim=0, keepdim=True)\n",
        "                sparsity_penalty = BETA * kl_divergence(rho, rho_hat)\n",
        "                loss = rec_loss + sparsity_penalty\n",
        "            else:\n",
        "                loss = rec_loss\n",
        "            # ---------------backward------------------\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            total_val_loss = []\n",
        "            for i, test_X in enumerate(testloader):\n",
        "                test_input_x, test_output_x = test_X\n",
        "                test_input_x = list_to_gpu(test_input_x, device)\n",
        "                test_output_x = list_to_gpu(test_output_x, device)\n",
        "                _, _, _, val_out = model(test_input_x)\n",
        "\n",
        "                val_loss, _ = criterion_for_list(criterion, output_x, out)\n",
        "                total_val_loss.append(val_loss.item())\n",
        "            epochs.set_description(\"Validation Loss: %g\" % round(np.mean(total_val_loss), 4))\n",
        "        if val_loss < best_loss:\n",
        "            no_improvement = 0\n",
        "            best_loss = val_loss\n",
        "            torch.save(model.state_dict(), \"best_model.pkl\")\n",
        "        else:\n",
        "            no_improvement += 1\n",
        "            if no_improvement == early_stopping_limit:\n",
        "                epochs.close()\n",
        "                break\n",
        "\n",
        "    # load functional labels of proteins\n",
        "    GO = sio.loadmat(args.data_folder + args.dataset + \"/\" +\n",
        "                     args.annotations_path + args.dataset + '_annotations.mat')\n",
        "\n",
        "    input_x = list_to_cpu(input_x)\n",
        "    output_x = list_to_cpu(output_x)\n",
        "    test_input_x = list_to_cpu(test_input_x)\n",
        "    test_output_x = list_to_cpu(test_output_x)\n",
        "\n",
        "    model.load_state_dict(torch.load('best_model.pkl'))\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        features, _, _, _ = model(list_to_gpu(Nets, device))\n",
        "        features = features.cpu().detach().numpy()\n",
        "        features = minmax_scale(features)\n",
        "\n",
        "    for level in args.label_names:\n",
        "        print(\"### Running for level: %s\" % (level))\n",
        "        perf = cross_validation(features, GO[level],\n",
        "                                n_trials=10)\n",
        "        avg_micro = 0.0\n",
        "        for ii in range(0, len(perf['fmax'])):\n",
        "            print('%0.5f %0.5f %0.5f %0.5f\\n' %\n",
        "                  (perf['pr_micro'][ii], perf['pr_macro'][ii], perf['fmax'][ii], perf['acc'][ii]))\n",
        "            avg_micro += perf['pr_micro'][ii]\n",
        "        avg_micro /= len(perf['fmax'])\n",
        "        print(\"### Average (over trials): m-AUPR = %0.3f\" % (avg_micro))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}